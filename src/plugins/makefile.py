# Copyright (C) 2010 Association of Universities for Research in Astronomy(AURA)
# 
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
# 
#     1. Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
# 
#     2. Redistributions in binary form must reproduce the above
#       copyright notice, this list of conditions and the following
#       disclaimer in the documentation and/or other materials provided
#       with the distribution.
# 
#     3. The name of AURA and its representatives may not be used to
#       endorse or promote products derived from this software without
#       specific prior written permission.
# 
# THIS SOFTWARE IS PROVIDED BY AURA ``AS IS'' AND ANY EXPRESS OR IMPLIED
# WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL AURA BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
# OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
# TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
# USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
# DAMAGE.
"""
Define all Makefile-specific properties here.

This is a bit of a hack... err... proof of concept, I mean ;-)
"""
import os

import jinja2

from eunomia import job





def _extract_num_instances(ad):
    for line in ad.split('\n'):
        if(not line.startswith('Queue')):
            continue
        
        tokens = line.strip().split()
        if(len(tokens) == 2):
            return(int(tokens[1]))
        return(1)
    raise(Exception('Unable to parse job ClassAd:\n %s' % (ad)))


def _extract_inouts(arg_string):
    args = arg_string.strip().split()
    input = ''
    output = ''
    
    nargs = len(args)
    for i in range(nargs):
        if(args[i] == '-i' and i < nargs):
            input = args[i+1]
        elif(args[i] == '-o' and i < nargs):
            output = args[i+1]
    return(input, output)
    


def _parse(dag, dir):
    # A DAG syntax is pretty simple
    # JOB JOBNAME JOBSCRIPT
    # PARENT JOBNAME CHILD JOBNAME
    # We do not support DATA jobs quite yet.
    dag = os.path.join(dir, dag)
    lines = [l.strip() for l in dag.split('\n') if l.strip()]
    
    # Nodes.
    nodes = {}                                                  # {name, Node}
    for line in lines:
        if(not line.startswith('JOB')):
            continue
        
        typ, name, script = line.split()
        nodes[name] = Node(name=name, 
                           script=os.path.join(dir, script),
                           children=[])
    
    # Relations.
    for line in lines:
        if(not line.startswith('PARENT')):
            continue
        
        # parent, name, child, child1 child2 child3...
        tokens = line.split()
        parent = nodes[tokens[1]]
        parent.children = [nodes[n] for n in tokens[3:]]
        for child in parent.children:
            child.parents.append(parent)
    return(nodes.values())


def _escape(arg_string):
    """
    Shell escape arguments.
    
    http://stackoverflow.com/questions/35817/how-to-escape-os-system-calls-in-python
    """
    args = arg_string.strip().split()
    return(' '.join(["'" + s.replace("'", "'\\''") + "'" for s in args]))



class Node(object):
    def __init__(self, name, script, children=[], parents=[]):
        ad = open(script).read()
        n = _extract_num_instances(ad)
        
        self.name = name
        self.job = job.Job.newFromClassAd(ad)
        self.job.Instances = n
        self.children = children
        self.parents = parents
        return    



class DAG(object):
    @classmethod
    def newFromDAG(cls, dag, dir):
        """
        Given a DAG text, parse it and create the corresponding DAG instance.
        """
        return(cls(nodes=_parse(dag, dir)))
    
    def __init__(self, nodes):
        self.nodes = nodes
        return
    
    
    def to_makefile(self):
        header = '\n'
        header += '# \n'
        header += '# Automatically generated by Eunomia\n'
        header += '# \n'
        phony = 'all: %s\n\n'
        phony += 'clean:\n'
        phony += '\trm -f %s\n\n'
        makefile = ''
        
        max_instances = max([n.job.Instances for n in self.nodes])
        
        # Turn the DAG into a Makefile. Try and understand what the final 
        # dataset is and create a 'all' rule for it. That one should be the 
        # output of the node with no children (true for simple cases).
        last = None
        all_outputs = ''
        for node in self.nodes:
            # Determine inputs and outputs
            input, output = _extract_inouts(node.job.Arguments)
            if(not node.children):
                last = output
            
            # Do job and argument expansion based on the number of instances.
            for i in range(node.job.Instances):
                # Make a copy.
                job_input = input.replace('$(Process)', str(i))
                job_output = output.replace('$(Process)', str(i))
                
                # Do the same processing to the full arg string.
                job_args = node.job.Arguments.replace('$(Process)', str(i))
                
                # FIXME: Does this happen if node.job.Instances == 1?
                if('%(ccdId)s' in job_input):
                    job_input = ' '.join([job_input % {'ccdId': j} 
                                          for j in range(max_instances)])
                if('%(ccdId)s' in job_output):
                    job_output = ' '.join([job_output % {'ccdId': j} 
                                           for j in range(max_instances)])
                
                makefile += job_output + ': ' + job_input + '\n'
                makefile += '\t%s %s\n\n' % (node.job.Executable, 
                                             _escape(job_args))
                all_outputs += job_output + ' '
        
        # Now write the all rule.
        return(header + phony % (last, all_outputs) + makefile)



def submit(dagName, workDir):
    """
    Write out a makefile with all the necessary targets to execute the Workflow
    serially (or in parallel wherever possible using make -j N).
    """
    # All the files we need (the .dag file and the .job files) are in `workDir`
    # and have the names defined in the .dag file (which we are given as 
    # `dagName`). So, first thing is to parse `dagName`.
    dag = DAG.newFromDAG(open(os.path.join(workDir, dagName)).read(), workDir)
    
    # Create the makefile
    f = open(os.path.join(workDir, 'Makefile'), 'w')
    f.write(dag.to_makefile())
    f.close()
    
    print('Makefile written in work directory %s' % (workDir))
    return(0)


