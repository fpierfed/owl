# Copyright (C) 2010 Association of Universities for Research in Astronomy(AURA)
# 
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
# 
#     1. Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
# 
#     2. Redistributions in binary form must reproduce the above
#       copyright notice, this list of conditions and the following
#       disclaimer in the documentation and/or other materials provided
#       with the distribution.
# 
#     3. The name of AURA and its representatives may not be used to
#       endorse or promote products derived from this software without
#       specific prior written permission.
# 
# THIS SOFTWARE IS PROVIDED BY AURA ``AS IS'' AND ANY EXPRESS OR IMPLIED
# WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL AURA BE LIABLE FOR ANY DIRECT, INDIRECT,
# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS
# OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
# TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
# USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH
# DAMAGE.
"""
Define all Makefile-specific properties here.

This is a bit of a hack... err... proof of concept, I mean ;-)
"""
import os

import jinja2

from eunomia import dag
from eunomia import job





class DAG(dag.DAG):
    def to_xgrid_plist(self):
        header = '\n'
        header += '# \n'
        header += '# Automatically generated by Eunomia\n'
        header += '# \n'
        phony = 'all: %s\n\n'
        phony += 'clean:\n'
        phony += '\trm -f %s\n\n'
        makefile = ''
        
        max_instances = max([n.job.Instances for n in self.nodes])
        
        # Turn the DAG into a Makefile. Try and understand what the final 
        # dataset is and create a 'all' rule for it. That one should be the 
        # output of the node with no children (true for simple cases).
        last = None
        all_outputs = ''
        for node in self.nodes:
            # Determine inputs and outputs
            input, output = _extract_inouts(node.job.Arguments)
            if(not node.children):
                last = output
            
            # Do job and argument expansion based on the number of instances.
            for i in range(node.job.Instances):
                # Make a copy.
                job_input = input.replace('$(Process)', str(i))
                job_output = output.replace('$(Process)', str(i))
                
                # Do the same processing to the full arg string.
                job_args = node.job.Arguments.replace('$(Process)', str(i))
                
                # FIXME: Does this happen if node.job.Instances == 1?
                if('%(ccdId)s' in job_input):
                    job_input = ' '.join([job_input % {'ccdId': j} 
                                          for j in range(max_instances)])
                if('%(ccdId)s' in job_output):
                    job_output = ' '.join([job_output % {'ccdId': j} 
                                           for j in range(max_instances)])
                
                makefile += job_output + ': ' + job_input + '\n'
                makefile += '\t%s %s\n\n' % (node.job.Executable, 
                                             _escape(job_args))
                all_outputs += job_output + ' '
        
        # Now write the all rule.
        return(header + phony % (last, all_outputs) + makefile)



def submit(dagName, workDir):
    """
    Write out a makefile with all the necessary targets to execute the Workflow
    serially (or in parallel wherever possible using make -j N).
    """
    # All the files we need (the .dag file and the .job files) are in `workDir`
    # and have the names defined in the .dag file (which we are given as 
    # `dagName`). So, first thing is to parse `dagName`.
    dag = DAG.newFromDAG(open(os.path.join(workDir, dagName)).read(), workDir)
    
    # Create the makefile
    f = open(os.path.join(workDir, 'XGrid.plist'), 'w')
    f.write(dag.to_xgrid_plist())
    f.close()
    
    print('XGrid batch job file written in work directory %s' % (workDir))
    return(0)


